{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cbe35c-95a1-4d9b-a6be-93dace5f2443",
   "metadata": {},
   "source": [
    "#### 三方库的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50211441-50bf-4cd8-aac2-f68944acca78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch.nn import Parameter\n",
    "import pdb\n",
    "import time\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax, degree, dropout_adj\n",
    "from torch_geometric.nn.inits import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb363b3-77aa-47a5-b6ff-303fa1291517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)          # Python\n",
    "    np.random.seed(seed)       # NumPy\n",
    "    torch.manual_seed(seed)    # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)          # PyTorch GPU（单卡）\n",
    "    torch.cuda.manual_seed_all(seed)      # PyTorch GPU（多卡）\n",
    "    torch.backends.cudnn.deterministic = True   # 保证卷积等算子确定性\n",
    "    torch.backends.cudnn.benchmark = False      # 关闭自动算法优化\n",
    "\n",
    "# 用法\n",
    "set_seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba6beb-cda1-46b7-a4a4-cc78d690637d",
   "metadata": {},
   "source": [
    "#### 训练数据的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051176dc-0518-4df1-a955-8221bcf25ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "'''\n",
    "    自定义PyTorch数据集加载类\n",
    "        继承Dataset类\n",
    "        重写__len__和__getitem__方法\n",
    "'''\n",
    "\n",
    "\n",
    "class DataLoad(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(DataLoad, self).__init__()\n",
    "        self.data = np.load(path+'train_user_pos_neg.npy', allow_pickle=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user, pos_item, neg_item = self.data[index]\n",
    "        return [user, pos_item, neg_item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6f9e5-8f2b-443a-95f6-d91b4e18a9da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 卷积层代码\n",
    "#### （1）单层卷积过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010846f0-8518-4d0d-9f91-2a55f40b780b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Base_gcn(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_index_weight = None, normalize=True, bias=True, aggr='add', **kwargs):\n",
    "        super(Base_gcn, self).__init__(aggr=aggr, **kwargs)\n",
    "        self.aggr = aggr\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        # self.edge_index_weight = kwargs['edge_index_weight']\n",
    "        self.edge_index_weight = edge_index_weight\n",
    "\n",
    "    def forward(self, x, edge_index, size=None):\n",
    "        if size is None:\n",
    "            edge_index, _ = remove_self_loops(edge_index)     # 剔除自环边\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, edge_attr=self.edge_index_weight)\n",
    "\n",
    "    def message(self, x_j, edge_index, size, edge_attr):\n",
    "        return torch.mul(x_j, edge_attr.view(-1, 1))\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr(self):\n",
    "        return '{}({},{})'.format(self.__class__.__name__, self.in_channels, self.out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc959c3-6468-4d4f-ab4f-31390403b5b4",
   "metadata": {},
   "source": [
    "#### （2）多层卷积处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d127b1-cbf6-4bed-b37e-7bb852255562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCNs(torch.nn.Module):\n",
    "    def __init__(self, edge_index, num_user, num_item, dim_id, num_layers, log_base, dim_latent=None, device=None, data_path='./'):\n",
    "        super(GCNs, self).__init__()\n",
    "        self.edge_index = edge_index\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.dim_id = dim_id\n",
    "        self.dim_latent = dim_latent\n",
    "        self.device = device\n",
    "        self.data_path = data_path\n",
    "        self.num_layers = num_layers\n",
    "        self.log_base = log_base\n",
    "        self.conv_embeds = []               # 卷积层对象组成的列表，储存三层卷积操作对象\n",
    "        self.edge_index_weight_item = None\n",
    "        self.edge_index_weight_user = None\n",
    "        self.edge_index_weight_USER = None\n",
    "        self.edge_index_weight_item = np.load(self.data_path + '/train_log_i.npy', allow_pickle=True)\n",
    "        self.change_base()\n",
    "        self.edge_index_weight_item = torch.tensor(self.edge_index_weight_item, dtype=torch.float)\n",
    "        self.edge_index_weight = torch.cat((self.edge_index_weight_item, self.edge_index_weight_item)).to(self.device)\n",
    "\n",
    "        if self.dim_latent:\n",
    "            self.embedding_user = torch.nn.Embedding(\n",
    "                num_embeddings=self.num_user, embedding_dim=self.dim_latent)\n",
    "            self.embedding_item = torch.nn.Embedding(\n",
    "                num_embeddings=self.num_item, embedding_dim=self.dim_latent)\n",
    "            nn.init.xavier_uniform_(self.embedding_user.weight, gain=1)\n",
    "            nn.init.xavier_uniform_(self.embedding_item.weight, gain=1)\n",
    "\n",
    "            self.conv_embed = Base_gcn(self.dim_latent, self.dim_latent, aggr='mean', edge_index_weight=self.edge_index_weight)\n",
    "    \n",
    "    def change_base(self):\n",
    "        if self.log_base != 1:\n",
    "            # 对log_(i)进行对数函数的换底\n",
    "            if self.edge_index_weight_item is not None:\n",
    "                self.edge_index_weight_item = self.edge_index_weight_item - 1\n",
    "                base = np.log(self.log_base*math.e)\n",
    "                self.edge_index_weight_item = self.edge_index_weight_item / base\n",
    "                self.edge_index_weight_item = self.edge_index_weight_item + 1\n",
    "            # 对log_(u)进行对数函数的换底\n",
    "            if self.edge_index_weight_user is not None:\n",
    "                base = np.log(self.log_base*math.e)\n",
    "                self.edge_index_weight_user = self.edge_index_weight_user / base\n",
    "            # 对log_(U)进行对数函数的换底\n",
    "            if self.edge_index_weight_USER is not None:\n",
    "                base = np.log(self.log_base*math.e)\n",
    "                self.edge_index_weight_USER = self.edge_index_weight_USER / base\n",
    "    \n",
    "    \n",
    "    def forward(self):\n",
    "        users_emb = self.embedding_user.weight\n",
    "        items_emb = self.embedding_item.weight\n",
    "        all_emb = torch.cat([users_emb, items_emb])     \n",
    "        embs = [all_emb]\n",
    "        for layer in range(self.num_layers):\n",
    "            all_emb = self.conv_embed(all_emb, self.edge_index)\n",
    "            embs.append(all_emb)\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        light_out = torch.mean(embs, dim=1)\n",
    "\n",
    "        return light_out, self.embedding_user, self.embedding_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf893f-1546-4dc1-bd37-5d4cd25da458",
   "metadata": {},
   "source": [
    "### 指标计算代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd85ee5-f243-42c1-83d3-34234d847020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(test_data, pred_data):\n",
    "    r = []\n",
    "    for i in range(len(test_data)):\n",
    "        groundTrue = test_data[i]\n",
    "        predictTopK = pred_data[i]\n",
    "        pred = list(map(lambda x: x in groundTrue, predictTopK))      # 两者同时存在的元素即为命中元素\n",
    "        pred = np.array(pred).astype(\"float\")\n",
    "        r.append(pred)\n",
    "    return np.array(r).astype('float')\n",
    "\n",
    "def RecallPrecision_ATk(test_data, r, k):\n",
    "    \"\"\"\n",
    "    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n",
    "    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n",
    "    k : top-k\n",
    "    \"\"\"\n",
    "    right_pred = r[:, :k].sum(1)       # 命中个数\n",
    "    precis_n = k\n",
    "    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n",
    "    recall = np.sum(right_pred/recall_n)\n",
    "    precis = np.sum(right_pred)/precis_n\n",
    "    return {'recall': recall, 'precise': precis}\n",
    "\n",
    "def NDCGatK_r(test_data,r,k):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain\n",
    "    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n",
    "    \"\"\"\n",
    "    assert len(r) == len(test_data)\n",
    "    pred_data = r[:, :k]\n",
    "\n",
    "    test_matrix = np.zeros((len(pred_data), k))\n",
    "    for i, items in enumerate(test_data):\n",
    "        length = k if k <= len(items) else len(items)\n",
    "        test_matrix[i, :length] = 1\n",
    "    max_r = test_matrix\n",
    "    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n",
    "    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n",
    "    dcg = np.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg/idcg\n",
    "    ndcg[np.isnan(ndcg)] = 0.\n",
    "    return np.sum(ndcg)\n",
    "\n",
    "def test_one_batch( X):\n",
    "    sorted_items = X[0].numpy()\n",
    "    groundTrue = X[1]\n",
    "    r = getLabel(groundTrue, sorted_items)           # groundTrue为测试的真实标签，sorted_items为预测标签\n",
    "    pre, recall, ndcg = [], [], []\n",
    "    for k in [5,10,15,20]:\n",
    "        ret = RecallPrecision_ATk(groundTrue, r, k)\n",
    "        pre.append(ret['precise'])\n",
    "        recall.append(ret['recall'])\n",
    "        ndcg.append(NDCGatK_r(groundTrue,r,k))\n",
    "    return {'recall':np.array(recall), \n",
    "            'precise':np.array(pre), \n",
    "            'ndcg':np.array(ndcg)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4971b-3d42-4aba-b176-a086e5147585",
   "metadata": {},
   "source": [
    "#### BoostGCN模型代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee2e541-c133-4570-90db-22521a88a821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "class BoostGCN(torch.nn.Module):\n",
    "    def __init__(self, edge_index, batch_size, num_user, num_item, num_neg, dim_x, reg_weight, num_layers, log_base, device=None, data_path='./'):\n",
    "        super(BoostGCN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.reg_weight = reg_weight\n",
    "        self.num_layers = num_layers\n",
    "        self.log_base = log_base\n",
    "        self.device = device\n",
    "        self.data_path = data_path\n",
    "        self.dim_latent = dim_x\n",
    "        self.CORES = multiprocessing.cpu_count() // 2\n",
    "        \n",
    "        self.edge_index = torch.tensor(edge_index, dtype=torch.int64).t().contiguous().to(self.device)       # 转置并使其在内存中连续存储\n",
    "        self.edge_index = torch.cat((self.edge_index, self.edge_index[[1, 0]]), dim=1).to(self.device)      # (2, num_edge*2)   edge_index[0]source结点\n",
    "\n",
    "        self.gcns = GCNs(self.edge_index, num_user, num_item, dim_x, num_layers=self.num_layers, log_base=self.log_base, dim_latent=self.dim_latent, device=self.device, data_path=self.data_path)\n",
    "        self.id_embedding = nn.init.xavier_normal_(torch.rand((num_user + num_item, dim_x), requires_grad=True)).to(self.device)\n",
    "        self.result_embed = nn.init.xavier_normal_(torch.rand((num_user + num_item, self.dim_latent))).to(self.device)\n",
    "\n",
    "    def forward(self, user_nodes, pos_item_nodes, neg_item_nodes):\n",
    "        \n",
    "        representation, users_emb, items_emb = self.gcns()\n",
    "        item_rep = representation[self.num_user:]\n",
    "        user_rep = representation[:self.num_user]\n",
    "        \n",
    "        self.result_embed = torch.cat((user_rep, item_rep), dim=0)\n",
    "        user_tensor = self.result_embed[user_nodes]\n",
    "        pos_item_tensor = self.result_embed[pos_item_nodes]\n",
    "        neg_item_tensor = self.result_embed[neg_item_nodes]\n",
    "        pos_scores = torch.sum(user_tensor * pos_item_tensor, dim=1)\n",
    "        neg_scores = torch.sum(user_tensor * neg_item_tensor, dim=1)\n",
    "        return pos_scores, neg_scores, users_emb, items_emb\n",
    "\n",
    "    def loss(self, data):\n",
    "        user, pos_items, neg_items = data\n",
    "        pos_scores, neg_scores, users_emb, items_emb = self.forward(user.to(self.device), pos_items.to(self.device), neg_items.to(self.device))\n",
    "        loss_value = -torch.mean(torch.log2(torch.sigmoid(pos_scores - neg_scores)))\n",
    "        userEmb = users_emb(user.to(self.device))\n",
    "        posEmb = items_emb((pos_items - self.num_user).to(self.device))\n",
    "        negEmb = items_emb((neg_items - self.num_user).to(self.device))\n",
    "        reg_loss = (1 / 2) * (userEmb.norm(2).pow(2) + posEmb.norm(2).pow(2) + negEmb.norm(2).pow(2)) / float(len(user))\n",
    "        reg_loss = self.reg_weight * (reg_loss)\n",
    "        return loss_value + reg_loss\n",
    "\n",
    "    def minibatch(self, tensors, **kwargs):\n",
    "\n",
    "        batch_size = kwargs.get('batch_size', 512)   #若未赋值则默认为512\n",
    "\n",
    "        if len(tensors) == 1:\n",
    "            tensor = tensors[0]\n",
    "            for i in range(0, len(tensor), batch_size):\n",
    "                yield tensor[i:i + batch_size]\n",
    "        else:\n",
    "            for i in range(0, len(tensors), batch_size):\n",
    "                yield tensors[i:i + batch_size]\n",
    "                \n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1]) # 在用户项目稀疏交互矩阵中，取出用户的已交互项目的索引，返回的是一个一维的 numpy 数组\n",
    "        return posItems\n",
    "\n",
    "    def accuracy(self, dataset, num_neg, batch_size=512, multicore=0, topks=[5,10,15,20]):\n",
    "        max_K = max(topks)                    # 取最大的topk值\n",
    "        if multicore == 1:                    # 是否开启多线程\n",
    "            pool = multiprocessing.Pool(self.CORES)\n",
    "        # 初始化各指标数值\n",
    "        results = {'precise': np.zeros(len(topks)), \n",
    "                   'recall': np.zeros(len(topks)), \n",
    "                   'ndcg': np.zeros(len(topks))\n",
    "                  }\n",
    "        users = list(range(len(dataset)))\n",
    "        try:\n",
    "            assert batch_size <= len(users) / 10\n",
    "        except AssertionError:\n",
    "            print(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n",
    "            \n",
    "        users_list = []\n",
    "        rating_list = []\n",
    "        groundTrue_list = []\n",
    "        total_batch = len(users) // batch_size + 1\n",
    "        bar = tqdm(total=total_batch)\n",
    "        for batch_users in self.minibatch(users, batch_size=batch_size):\n",
    "            bar.update(1)\n",
    "            # 取出每个用户除测试/验证集正负样本之外的所有项目，返回的是一个一维的 numpy 数组\n",
    "            except_items = []\n",
    "            for user in batch_users:\n",
    "                # 需要计算交互的项目的索引位置\n",
    "                test_items = np.array(dataset[user][1:])-self.num_user\n",
    "                except_items.append(test_items)\n",
    "            # 取出测试集用户交互项目(嵌套列表)   \n",
    "            groundTrue = [list(np.array(dataset[user][1:-num_neg])-self.num_user) for user in batch_users]\n",
    "            batch_users_gpu = torch.Tensor(batch_users).long()\n",
    "            batch_users_gpu = batch_users_gpu.to(self.device)\n",
    "            # 获得批量用户和所有项目的embedding(多层聚合后的)交互得分\n",
    "            rating = torch.matmul(self.result_embed[batch_users_gpu], self.result_embed[self.num_user:].t()).to(self.device)\n",
    "                   \n",
    "            for index in range(len(except_items)):\n",
    "                rating[index][list(except_items[index])] += 10240     # 将测试集中的交互得分统一加上10240\n",
    "            _, rating_K = torch.topk(rating, k=max_K)\n",
    "            rating = rating.detach().cpu().numpy()\n",
    "            del rating\n",
    "            users_list.append(batch_users)\n",
    "            rating_list.append(rating_K.cpu())\n",
    "            groundTrue_list.append(groundTrue)\n",
    "        assert total_batch == len(users_list)\n",
    "        X = zip(rating_list, groundTrue_list)\n",
    "        if multicore == 1:\n",
    "            pre_results = pool.map(test_one_batch, X)\n",
    "        else:\n",
    "            pre_results = []\n",
    "            for x in X:\n",
    "                pre_results.append(test_one_batch(x))\n",
    "        for result in pre_results:\n",
    "            results['recall'] += result['recall']\n",
    "            results['precise'] += result['precise']\n",
    "            results['ndcg'] += result['ndcg']\n",
    "        results['recall'] /= float(len(users))\n",
    "        results['precise'] /= float(len(users))\n",
    "        results['ndcg'] /= float(len(users))\n",
    "        if multicore == 1:\n",
    "            pool.close()\n",
    "        bar.close()\n",
    "        return results \n",
    " \n",
    "    def accuracy_full_neg(self, dataset, len_datas, batch_size=512, multicore=0, topks=[5,10,15,20]):         # topk = 10, neg_num=1000  \n",
    "        max_K = max(topks)                    # 取最大的topk值\n",
    "        if multicore == 1:                    # 是否开启多线程\n",
    "            pool = multiprocessing.Pool(self.CORES)\n",
    "        # 初始化各指标数值\n",
    "        results = {'precise': np.zeros(len(topks)), \n",
    "                   'recall': np.zeros(len(topks)), \n",
    "                   'ndcg': np.zeros(len(topks))\n",
    "                  }\n",
    "        users = list(range(len(dataset)))\n",
    "        try:\n",
    "            assert batch_size <= len(users) / 10\n",
    "        except AssertionError:\n",
    "            print(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n",
    "            \n",
    "        users_list = []\n",
    "        rating_list = []\n",
    "        groundTrue_list = []\n",
    "        total_batch = len(users) // batch_size + 1\n",
    "        bar = tqdm(total=total_batch)\n",
    "        for batch_users in self.minibatch(users, batch_size=batch_size):\n",
    "            bar.update(1)\n",
    "            # 取出每个用户除测试/验证集正负样本之外的所有项目，返回的是一个一维的 numpy 数组            \n",
    "            except_items = []\n",
    "            for user in batch_users:\n",
    "                all_items = np.arange(self.num_item)\n",
    "                # 需要删除的索引位置\n",
    "                test_items = np.array(dataset[user][1:])-self.num_user\n",
    "                # 创建布尔掩码，表示保留哪些索引位置的元素\n",
    "                mask = np.ones(len(all_items), dtype=bool)\n",
    "                mask[test_items] = False\n",
    "                # 使用布尔掩码来筛选数组\n",
    "                save_items = all_items[mask]\n",
    "                except_items.append(save_items)          \n",
    "                        \n",
    "            # 取出测试集用户交互项目(嵌套列表)   \n",
    "            groundTrue = [list(np.array(dataset[user][1:len_datas[user]+1])-self.num_user) for user in batch_users]\n",
    "            batch_users_gpu = torch.Tensor(batch_users).long()\n",
    "            batch_users_gpu = batch_users_gpu.to(self.device)\n",
    "            # 获得批量用户和所有项目的embedding(多层聚合后的)交互得分\n",
    "            rating = torch.matmul(self.result_embed[batch_users_gpu], self.result_embed[self.num_user:].t()).to(self.device)\n",
    "\n",
    "            # rating[exclude_index, exclude_items] = -(1<<10)           # 将测试集之外的交互得分设置为-1024\n",
    "            for index in range(len(except_items)):\n",
    "                rating[index][list(except_items[index])] = -10240\n",
    "            _, rating_K = torch.topk(rating, k=max_K)\n",
    "            rating = rating.detach().cpu().numpy()\n",
    "            # rating = rating.detach().numpy()\n",
    "            del rating\n",
    "            users_list.append(batch_users)\n",
    "            rating_list.append(rating_K.cpu())\n",
    "            groundTrue_list.append(groundTrue)\n",
    "        assert total_batch == len(users_list)\n",
    "        X = zip(rating_list, groundTrue_list)\n",
    "        if multicore == 1:\n",
    "            pre_results = pool.map(test_one_batch, X)\n",
    "        else:\n",
    "            pre_results = []\n",
    "            for x in X:\n",
    "                pre_results.append(test_one_batch(x))\n",
    "        for result in pre_results:\n",
    "            results['recall'] += result['recall']\n",
    "            results['precise'] += result['precise']\n",
    "            results['ndcg'] += result['ndcg']\n",
    "        results['recall'] /= float(len(users))\n",
    "        results['precise'] /= float(len(users))\n",
    "        results['ndcg'] /= float(len(users))\n",
    "        if multicore == 1:\n",
    "            pool.close()\n",
    "        bar.close()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ca4d2-6701-43cb-b972-70b638414ced",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7269719a-8816-4376-ada7-09ddc8e88c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\")                 # 使用gpu设备cuda:0\n",
    "        self.data_path = '/Data/yelp2018/'                       # 数据集路径\n",
    "        self.learning_rate = 0.001                           # 学习率\n",
    "        self.weight_decay = 1e-6                             # 权重衰退:本质上是一个 L2正则化系数,解决过拟合问题\n",
    "        self.batch_size = 1024                               # 批训练\n",
    "        self.num_layers = 4                                # 图卷积层数\n",
    "        self.log_base = 1                                  # 对数函数的底数(e的倍数)，用于消融实验\n",
    "        self.num_workers = 2                               # 训练数据加载器DataLoader中用于数据加载的子进程数量\n",
    "        self.num_epoch = 1000                              # 训练轮次\n",
    "        self.num_user = 31668                                   # 用户节点数量\n",
    "        self.num_item = 38048                                   # 项目节点数量\n",
    "        self.num_neg = 1500\n",
    "        self.PATH_weight_save = True\n",
    "        self.dim_latent = 64\n",
    "        self.update_epoch_num = 0                          # 记录已经多少轮模型没有得到更新\n",
    "        self.save_best_model = 'best_model.pth'               # 保存验证集最佳模型\n",
    "        \n",
    "        print('Loading data  ...')\n",
    "        # 即两种正负样本\n",
    "        self.train_dataset = DataLoad(self.data_path)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "        self.edge_index = np.load(self.data_path + 'train_edge_index.npy', allow_pickle=True)\n",
    "        self.val_dataset = np.load(self.data_path + 'val_data.npy', allow_pickle=True)\n",
    "        print('Data has been loaded.')\n",
    "\n",
    "        self.model = BoostGCN(self.edge_index, self.batch_size, self.num_user, self.num_item, self.num_neg, self.dim_latent, self.weight_decay, self.num_layers, self.log_base, self.device, self.data_path)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "    def run(self):\n",
    "        max_recall = 0.\n",
    "        \n",
    "        df = pd.DataFrame(columns=['epoch', 'Loss', 'precise@5', 'recall@5', 'ndcg@5', 'precise@10', 'recall@10', \n",
    "                                   'ndcg@10', 'precise@15', 'recall@15', 'ndcg@15', 'precise@20', 'recall@20', 'ndcg@20', 'train_time', 'test_time'])\n",
    "        df.to_csv(\"./val_res_BoostGCN.csv\", index=False)  # 路径可以根据需要更改\n",
    "        \n",
    "        for epoch in range(self.num_epoch):\n",
    "            if self.update_epoch_num >= 10:\n",
    "                break\n",
    "            self.update_epoch_num = self.update_epoch_num + 1\n",
    "            self.model.train()\n",
    "            sum_loss = 0.0\n",
    "            start_time = time.perf_counter()   # 打印时间以秒为单位\n",
    "            for data in self.train_dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                self.loss = self.model.loss(data)\n",
    "\n",
    "                self.loss.backward()\n",
    "                self.optimizer.step()\n",
    "                sum_loss += self.loss\n",
    "            end_time = time.perf_counter()\n",
    "            # 计算时间差,得到模型每轮训练时间\n",
    "            train_time = end_time - start_time\n",
    "            current_loss = sum_loss.item() / self.batch_size\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                start_time = time.perf_counter()\n",
    "                evaluations = self.model.accuracy(self.val_dataset, self.num_neg, multicore=1, batch_size=1024)\n",
    "                end_time = time.perf_counter()\n",
    "                # 计算时间差，得到指标计算时间\n",
    "                test_time = end_time - start_time\n",
    "            if evaluations['recall'][0] > max_recall:\n",
    "                self.update_epoch_num = 0      # 标志模型更新\n",
    "            print('{0}-th Loss:{1:.4f} precise@5:{2:.4f} recall@5:{3:.4f} ndcg@5:{4:.4f} precise@10:{5:.4f} recall@10:{6:.4f} ndcg@10:{7:.4f} precise@15:{8:.4f} recall@15:{9:.4f} ndcg@15:{10:.4f} precise@20:{11:.4f} recall@20:{12:.4f} ndcg@20:{13:.4f}'.format(epoch, current_loss, \n",
    "            evaluations['precise'][0], evaluations['recall'][0], evaluations['ndcg'][0], \n",
    "            evaluations['precise'][1], evaluations['recall'][1], evaluations['ndcg'][1], \n",
    "            evaluations['precise'][2], evaluations['recall'][2], evaluations['ndcg'][2], \n",
    "            evaluations['precise'][3], evaluations['recall'][3], evaluations['ndcg'][3]))\n",
    "\n",
    "            list = [epoch, current_loss, evaluations['precise'][0], evaluations['recall'][0], evaluations['ndcg'][0], \n",
    "                    evaluations['precise'][1], evaluations['recall'][1], evaluations['ndcg'][1], \n",
    "                    evaluations['precise'][2], evaluations['recall'][2], evaluations['ndcg'][2], \n",
    "                    evaluations['precise'][3], evaluations['recall'][3], evaluations['ndcg'][3], train_time, test_time]\n",
    "\n",
    "            # 由于DataFrame是Pandas库中的一种数据结构，它类似excel，是一种二维表，所以需要将list以二维列表的形式转化为DataFrame\n",
    "            data = pd.DataFrame([list])\n",
    "            # 3）将数据写入csv文件\n",
    "            data.to_csv('./val_res_BoostGCN.csv', mode='a', header=False, index=False)  # mode设为a,就可以向csv文件追加数据了\n",
    "\n",
    "            if self.PATH_weight_save and evaluations['recall'][0] > max_recall:\n",
    "                max_recall = evaluations['recall'][0]\n",
    "                torch.save(self.model, self.save_best_model)\n",
    "                print('module weights saved....')\n",
    "            \n",
    "            if epoch == 0:\n",
    "                torch.save(self.model, 'init_train_model.pth')\n",
    "                \n",
    "            print(f'EPOCH[{epoch+1}/{self.num_epoch}]')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5c92c5-d15b-453c-be4a-ade92b833201",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data  ...\n",
      "Data has been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th Loss:0.1164 precise@5:0.3086 recall@5:0.1901 ndcg@5:0.3338 precise@10:0.2540 recall@10:0.3033 ndcg@10:0.3467 precise@15:0.2197 recall@15:0.3849 ndcg@15:0.3713 precise@20:0.1953 recall@20:0.4490 ndcg@20:0.3938\n",
      "module weights saved....\n",
      "EPOCH[1/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss:0.0834 precise@5:0.3165 recall@5:0.1957 ndcg@5:0.3427 precise@10:0.2602 recall@10:0.3113 ndcg@10:0.3557 precise@15:0.2249 recall@15:0.3944 ndcg@15:0.3810 precise@20:0.1995 recall@20:0.4586 ndcg@20:0.4034\n",
      "module weights saved....\n",
      "EPOCH[2/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:45<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-th Loss:0.0681 precise@5:0.3217 recall@5:0.1993 ndcg@5:0.3477 precise@10:0.2633 recall@10:0.3152 ndcg@10:0.3603 precise@15:0.2267 recall@15:0.3979 ndcg@15:0.3852 precise@20:0.2010 recall@20:0.4625 ndcg@20:0.4077\n",
      "module weights saved....\n",
      "EPOCH[3/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-th Loss:0.0556 precise@5:0.3240 recall@5:0.2008 ndcg@5:0.3512 precise@10:0.2655 recall@10:0.3179 ndcg@10:0.3640 precise@15:0.2280 recall@15:0.3999 ndcg@15:0.3883 precise@20:0.2022 recall@20:0.4652 ndcg@20:0.4111\n",
      "module weights saved....\n",
      "EPOCH[4/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-th Loss:0.0462 precise@5:0.3188 recall@5:0.1972 ndcg@5:0.3450 precise@10:0.2605 recall@10:0.3120 ndcg@10:0.3570 precise@15:0.2241 recall@15:0.3934 ndcg@15:0.3814 precise@20:0.1986 recall@20:0.4571 ndcg@20:0.4035\n",
      "EPOCH[5/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-th Loss:0.0386 precise@5:0.3204 recall@5:0.1982 ndcg@5:0.3468 precise@10:0.2626 recall@10:0.3141 ndcg@10:0.3594 precise@15:0.2259 recall@15:0.3961 ndcg@15:0.3838 precise@20:0.2003 recall@20:0.4600 ndcg@20:0.4061\n",
      "EPOCH[6/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:44<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-th Loss:0.0330 precise@5:0.3164 recall@5:0.1957 ndcg@5:0.3428 precise@10:0.2594 recall@10:0.3098 ndcg@10:0.3551 precise@15:0.2235 recall@15:0.3916 ndcg@15:0.3797 precise@20:0.1982 recall@20:0.4555 ndcg@20:0.4020\n",
      "EPOCH[7/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:42<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-th Loss:0.0280 precise@5:0.3141 recall@5:0.1941 ndcg@5:0.3403 precise@10:0.2569 recall@10:0.3070 ndcg@10:0.3521 precise@15:0.2215 recall@15:0.3881 ndcg@15:0.3765 precise@20:0.1966 recall@20:0.4515 ndcg@20:0.3986\n",
      "EPOCH[8/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-th Loss:0.0241 precise@5:0.3205 recall@5:0.1985 ndcg@5:0.3472 precise@10:0.2618 recall@10:0.3131 ndcg@10:0.3591 precise@15:0.2254 recall@15:0.3953 ndcg@15:0.3837 precise@20:0.1996 recall@20:0.4585 ndcg@20:0.4057\n",
      "EPOCH[9/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:44<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9-th Loss:0.0212 precise@5:0.3207 recall@5:0.1989 ndcg@5:0.3476 precise@10:0.2628 recall@10:0.3146 ndcg@10:0.3603 precise@15:0.2261 recall@15:0.3972 ndcg@15:0.3850 precise@20:0.2005 recall@20:0.4616 ndcg@20:0.4075\n",
      "EPOCH[10/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-th Loss:0.0190 precise@5:0.3215 recall@5:0.1991 ndcg@5:0.3481 precise@10:0.2626 recall@10:0.3135 ndcg@10:0.3600 precise@15:0.2261 recall@15:0.3960 ndcg@15:0.3846 precise@20:0.2004 recall@20:0.4605 ndcg@20:0.4070\n",
      "EPOCH[11/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-th Loss:0.0166 precise@5:0.3196 recall@5:0.1975 ndcg@5:0.3460 precise@10:0.2609 recall@10:0.3116 ndcg@10:0.3575 precise@15:0.2252 recall@15:0.3946 ndcg@15:0.3825 precise@20:0.1998 recall@20:0.4587 ndcg@20:0.4048\n",
      "EPOCH[12/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:42<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-th Loss:0.0151 precise@5:0.3198 recall@5:0.1982 ndcg@5:0.3461 precise@10:0.2619 recall@10:0.3132 ndcg@10:0.3587 precise@15:0.2261 recall@15:0.3968 ndcg@15:0.3840 precise@20:0.2003 recall@20:0.4601 ndcg@20:0.4061\n",
      "EPOCH[13/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-th Loss:0.0143 precise@5:0.3156 recall@5:0.1951 ndcg@5:0.3424 precise@10:0.2586 recall@10:0.3088 ndcg@10:0.3547 precise@15:0.2230 recall@15:0.3906 ndcg@15:0.3792 precise@20:0.1973 recall@20:0.4529 ndcg@20:0.4008\n",
      "EPOCH[14/1000]\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd28c6e-b00e-48d1-8dae-70038505eb1d",
   "metadata": {},
   "source": [
    "### 加载最佳模型，进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f974d65-eb7e-4f52-b1f0-d29ef4fbec0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:43<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precise@5:0.3249 recall@5:0.2015 ndcg@5:0.3523 precise@10:0.2651 recall@10:0.3181 ndcg@10:0.3643 precise@15:0.2281 recall@15:0.4011 ndcg@15:0.3893 precise@20:0.2020 recall@20:0.4657 ndcg@20:0.4119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/Data/yelp2018/'\n",
    "test_dataset = np.load(data_path + 'test_data.npy', allow_pickle=True)\n",
    "num_neg = 1500\n",
    "\n",
    "df = pd.DataFrame(columns=['precise@5', 'recall@5', 'ndcg@5', 'precise@10', 'recall@10', 'ndcg@10', 'precise@15', \n",
    "                           'recall@15', 'ndcg@15', 'precise@20', 'recall@20', 'ndcg@20'])\n",
    "df.to_csv(\"./test_res_BoostGCN.csv\", index=False)  # 路径可以根据需要更改\n",
    "\n",
    "test_model = torch.load('best_model.pth')\n",
    "# 设置模型为评估模式\n",
    "test_model.eval()\n",
    "evaluations = test_model.accuracy(test_dataset, num_neg, batch_size=1024)\n",
    "\n",
    "print('precise@5:{0:.4f} recall@5:{1:.4f} ndcg@5:{2:.4f} precise@10:{3:.4f} recall@10:{4:.4f} ndcg@10:{5:.4f} precise@15:{6:.4f} recall@15:{7:.4f} ndcg@15:{8:.4f} precise@20:{9:.4f} recall@20:{10:.4f} ndcg@20:{11:.4f}'.format(\n",
    "evaluations['precise'][0], evaluations['recall'][0], evaluations['ndcg'][0], \n",
    "evaluations['precise'][1], evaluations['recall'][1], evaluations['ndcg'][1], \n",
    "evaluations['precise'][2], evaluations['recall'][2], evaluations['ndcg'][2], \n",
    "evaluations['precise'][3], evaluations['recall'][3], evaluations['ndcg'][3]))\n",
    "\n",
    "li = [evaluations['precise'][0], evaluations['recall'][0], evaluations['ndcg'][0], \n",
    "      evaluations['precise'][1], evaluations['recall'][1], evaluations['ndcg'][1], \n",
    "      evaluations['precise'][2], evaluations['recall'][2], evaluations['ndcg'][2], \n",
    "      evaluations['precise'][3], evaluations['recall'][3], evaluations['ndcg'][3]]\n",
    "\n",
    "# 由于DataFrame是Pandas库中的一种数据结构，它类似excel，是一种二维表，所以需要将list以二维列表的形式转化为DataFrame\n",
    "data = pd.DataFrame([li])\n",
    "# 3）将数据写入csv文件\n",
    "data.to_csv('./test_res_BoostGCN.csv', mode='a', header=False, index=False)  # mode设为a,就可以向csv文件追加数据了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163f764-69e0-4267-874c-8b84d1e8b6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
